{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yang-star-source/Deep-Image-Prior/blob/master/Deep_Image_Prior_Denoising.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Yang-star-source/Deep-Image-Prior.git"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OVBqs5ahnaFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCVS9m3o_EU8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,dropout_prob = 0.0 ):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.GroupNorm(32,in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1,padding_mode='reflect')\n",
        "        self.norm2 = nn.GroupNorm(32,out_channels)\n",
        "        self.drop = nn.Dropout(dropout_prob)\n",
        "        self.conv2 = nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1,padding_mode='reflect')\n",
        "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels,out_channels,kernel_size=1)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self,x):\n",
        "        x1 = x\n",
        "\n",
        "        x = self.norm1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = self.norm2(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.conv2(x)\n",
        "        return x + self.shortcut(x1)\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels):\n",
        "        super().__init__()\n",
        "        self.res1 = ResNetBlock(in_channels,out_channels)\n",
        "        self.res2 = ResNetBlock(out_channels,out_channels)\n",
        "        self.down = nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=2,padding=1,padding_mode='reflect')\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "\n",
        "        skip_connection = x\n",
        "\n",
        "        x = self.down(x)\n",
        "        return x , skip_connection\n",
        "\n",
        "class MidBlock(nn.Module):\n",
        "    def __init__(self,in_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.res1 = ResNetBlock(in_channels,in_channels)\n",
        "        self.res2 = ResNetBlock(in_channels,in_channels)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        return x\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(in_channels,in_channels,kernel_size=3,padding=1,padding_mode='reflect')\n",
        "        )\n",
        "        # Why input *2 ? Because we have to concatenate the channels\n",
        "        self.res1 = ResNetBlock(in_channels*2,out_channels)\n",
        "        self.res2 = ResNetBlock(out_channels,out_channels)\n",
        "\n",
        "    def forward(self,x,skip_connection):\n",
        "        x = self.up(x)\n",
        "\n",
        "        # cancatenate at channels dimension\n",
        "        x = torch.cat([x,skip_connection],dim=1)\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        return x\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self,in_channels=3,out_channels=3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.init_conv = nn.Conv2d(in_channels,64,kernel_size=3,padding=1,padding_mode='reflect')\n",
        "\n",
        "        self.down1 = DownBlock(64,64)\n",
        "        self.down2 = DownBlock(64,128)\n",
        "        self.down3 = DownBlock(128,128)\n",
        "        self.down4 = DownBlock(128,256)\n",
        "\n",
        "        self.mid = MidBlock(256)\n",
        "\n",
        "        self.up1 = UpBlock(256,128)\n",
        "        self.up2 = UpBlock(128,128)\n",
        "        self.up3 = UpBlock(128,64)\n",
        "        self.up4 = UpBlock(64,64)\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.GroupNorm(32,64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64,out_channels,kernel_size=3,padding=1,padding_mode='reflect')\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        x1 , skip1 = self.down1(x)\n",
        "        x2 , skip2 = self.down2(x1)\n",
        "        x3 , skip3 = self.down3(x2)\n",
        "        x4 , skip4 = self.down4(x3)\n",
        "\n",
        "        x = self.mid(x4)\n",
        "\n",
        "        x = self.up1(x,skip4)\n",
        "        x = self.up2(x,skip3)\n",
        "        x = self.up3(x,skip2)\n",
        "        x = self.up4(x,skip1)\n",
        "\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBytXHyzHjJN"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "IMAGE_PATH = \"/content/Deep-Image-Prior/images/0013.png\"\n",
        "img = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "img = transform(img)\n",
        "img = img.unsqueeze(0).to(DEVICE)\n",
        "print(img.shape)\n",
        "dataset_img = img\n",
        "\n",
        "beta_start = 0.0001\n",
        "beta_end = 0.02\n",
        "num_timesteps = 1000\n",
        "\n",
        "betas = torch.linspace(beta_start, beta_end, num_timesteps,device=DEVICE)\n",
        "alphas = 1.0 - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
        "\n",
        "beta =None\n",
        "sqrt_alphas_cumprod_t = None\n",
        "sqrt_one_minus_alphas_cumprod_t = None\n",
        "def get_noisy_img(img,t):\n",
        "    global beta,sqrt_alphas_cumprod_t,sqrt_one_minus_alphas_cumprod_t\n",
        "    beta = betas[t]\n",
        "    sqrt_alphas_cumprod_t = sqrt_alphas_cumprod[t]\n",
        "    sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod[t]\n",
        "    noise = torch.randn_like(img)\n",
        "    return sqrt_alphas_cumprod_t * img + sqrt_one_minus_alphas_cumprod_t * noise\n",
        "\n",
        "class TV_loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self,x):\n",
        "        horizontal_diff = x[:,:,1:,:] - x[:,:,:-1,:]\n",
        "        vertical_diff = x[:,:,:,1:] - x[:,:,:,:-1]\n",
        "        h_sum = torch.sum(torch.abs(horizontal_diff))\n",
        "        v_sum = torch.sum(torch.abs(vertical_diff))\n",
        "        return h_sum + v_sum\n",
        "\n",
        "def plottable_clean_img(img):\n",
        "    if img.dtype == torch.float32:\n",
        "        img = img.detach().cpu().squeeze(0).permute(1,2,0).numpy()\n",
        "        img = img * 0.5 + 0.5\n",
        "        img = np.clip(img,0,1)\n",
        "    return img\n",
        "\n",
        "\n",
        "\n",
        "noisy_img = get_noisy_img(img,torch.tensor(300,device=DEVICE)).to(DEVICE)\n",
        "\n",
        "\n",
        "def denoising_step(noisy_img):\n",
        "    model = Unet().to(DEVICE)\n",
        "\n",
        "    z = torch.randn(noisy_img.shape,device=DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(),lr=0.0001)\n",
        "\n",
        "    EPOCHS = 2000\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    global frames\n",
        "    frames=[]\n",
        "\n",
        "    best_loss = float(\"inf\")\n",
        "    best_img = torch.randn(3,256,256,device=DEVICE)\n",
        "    best_epoch = 0\n",
        "\n",
        "    print(\"Start Training\")\n",
        "    for epoch in range(EPOCHS):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        reconstructed_img = model(z)\n",
        "        mse_loss = F.mse_loss(reconstructed_img,noisy_img)\n",
        "\n",
        "        if epoch > 50:\n",
        "            tv_loss = TV_loss()(reconstructed_img)\n",
        "            loss = mse_loss + tv_loss * 1e-6\n",
        "        else:\n",
        "            loss = mse_loss\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if loss.item() < best_loss:\n",
        "            best_loss = loss.item()\n",
        "            best_img = reconstructed_img\n",
        "            best_epoch = epoch\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            frames.append(plottable_clean_img(reconstructed_img))\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "\n",
        "          clear_output(wait=True)\n",
        "          print(f\"Epoch {epoch} Loss {loss.item()}\")\n",
        "\n",
        "          fig , (ax_orig , ax_img , ax_loss , ax_best) = plt.subplots(1,4,figsize=(20,5))\n",
        "\n",
        "          original_img = plottable_clean_img(noisy_img)\n",
        "          ax_orig.imshow(original_img)\n",
        "          ax_orig.set_title(\"Original Image\")\n",
        "\n",
        "          current_img = plottable_clean_img(reconstructed_img)\n",
        "          ax_img.imshow(current_img)\n",
        "          ax_img.set_title(f\"Epoch {epoch}\")\n",
        "\n",
        "          ax_loss.plot(losses)\n",
        "          ax_loss.set_title(\"Loss\")\n",
        "\n",
        "          best_img = plottable_clean_img(best_img)\n",
        "          ax_best.imshow(best_img)\n",
        "          ax_best.set_title(f\"Best Image at Epoch :{best_epoch}\")\n",
        "\n",
        "          plt.show()\n",
        "\n",
        "denoising_step(noisy_img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UI CELL , VISUALIZATION OF CLEANER IMAGE"
      ],
      "metadata": {
        "id": "TsaorU9XrYO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "print(f\"Sqrt(1-alpha_cumprod) : {sqrt_one_minus_alphas_cumprod_t}\")\n",
        "print(f\"Sqrt(alpha_cumprod) : {sqrt_alphas_cumprod_t}\")\n",
        "print(len(frames))\n",
        "def show_frame(frame_idx):\n",
        "    fig , (ax_frames , ax_orig) = plt.subplots(1,2,figsize=(10,5))\n",
        "    ax_frames.imshow(frames[frame_idx])\n",
        "    ax_frames.axis('off')\n",
        "    ax_frames.set_title(f\"Frame {frame_idx} (Approx Iter {frame_idx*1})\")\n",
        "\n",
        "    ax_orig.imshow(plottable_clean_img(noisy_img))\n",
        "    ax_orig.axis('off')\n",
        "    ax_orig.set_title(\"Original Image\")\n",
        "    plt.show()\n",
        "\n",
        "# Create the slider\n",
        "slider = widgets.IntSlider(\n",
        "    value=0,\n",
        "    min=0,\n",
        "    max=len(frames)-1,\n",
        "    step=1,\n",
        "    description='Epoch:',\n",
        "    continuous_update=True\n",
        ")\n",
        "\n",
        "number_box = widgets.BoundedIntText(\n",
        "    value=0,\n",
        "    min=0,\n",
        "    max=len(frames)-1,\n",
        "    step=1,\n",
        "    description='Go to Frame:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "widgets.jslink((slider, 'value'), (number_box, 'value'))\n",
        "\n",
        "# 4. Create the Layout (Box on the left, Slider on the right)\n",
        "ui = widgets.HBox([number_box, slider])\n",
        "\n",
        "# 5. Connect to your plotting function\n",
        "# (We use interactive_output so the plot appears below the controls)\n",
        "out = widgets.interactive_output(show_frame, {'frame_idx': slider})\n",
        "\n",
        "# 6. Display\n",
        "display(ui, out)\n",
        "\n"
      ],
      "metadata": {
        "id": "HWmPiMXGUJ5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Change The frame_idx on which image you think is Cleanest"
      ],
      "metadata": {
        "id": "68J2CIqAq0tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "'''\n",
        "=======================================================================================================\n",
        "REMEMBER TO CHANGE THE FRAME IDX ON WHICH YOU THINK IS CLEANEST BASED ON FRAME_IDX IN THE PREVIOUS CELL\n",
        "=======================================================================================================\n",
        "'''\n",
        "frame_idx = 319\n",
        "frame = frames[frame_idx]\n",
        "print(frame.shape)\n",
        "\n",
        "fig ,ax = plt.subplots(1,1,figsize=(10,5))\n",
        "ax.imshow(frame)\n",
        "ax.axis('off')\n",
        "ax.set_title(f\"Frame {frame_idx} (Approx Iter {frame_idx*1})\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hgAU1FWyKA3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Datatype\n",
        "Numpy float [0,1] -> Numpy uint8 [0,255] -> PIL -> Tensor"
      ],
      "metadata": {
        "id": "tnKxF2S48nv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frame = (frame*255).astype(np.uint8)\n",
        "frame_pil = Image.fromarray(frame)\n",
        "blurry_img = transform(frame_pil).unsqueeze(0).to(DEVICE)\n",
        "print(blurry_img.shape)"
      ],
      "metadata": {
        "id": "W_L7_wFjcY2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If need denoise again , GET BACK TO RUN UI CELL to get cleaner image"
      ],
      "metadata": {
        "id": "s1kgjR0o-9yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "denoising_step(blurry_img)"
      ],
      "metadata": {
        "id": "eVbtbGno_BP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If Need to Sharpen Image"
      ],
      "metadata": {
        "id": "2dnAs1_--1Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,dropout_prob = 0.0):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "        self.norm1 = nn.GroupNorm(32,in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1)\n",
        "        self.norm2 = nn.GroupNorm(32,out_channels)\n",
        "        self.drop = nn.Dropout(dropout_prob)\n",
        "        self.conv2 = nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1)\n",
        "        self.silu = nn.SiLU()\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels,out_channels,kernel_size=1)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self,x):\n",
        "        x1 = x\n",
        "\n",
        "        x = self.norm1(x)\n",
        "        x = self.silu(x)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = self.norm2(x)\n",
        "        x = self.silu(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.conv2(x)\n",
        "        return x + self.shortcut(x1)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self,in_channels):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.norm = nn.GroupNorm(32,in_channels)\n",
        "\n",
        "        self.to_q = nn.Linear(in_channels,in_channels)\n",
        "        self.to_k = nn.Linear(in_channels,in_channels)\n",
        "        self.to_v = nn.Linear(in_channels,in_channels)\n",
        "\n",
        "        self.to_out = nn.Linear(in_channels,in_channels)\n",
        "\n",
        "    def forward(self,x):\n",
        "        residual = x\n",
        "        B,C,H,W = x.shape\n",
        "        x = self.norm(x)\n",
        "        x = x.view(B,C,-1).permute(0,2,1)\n",
        "\n",
        "        q = self.to_q(x)\n",
        "        k = self.to_k(x)\n",
        "        v = self.to_v(x)\n",
        "\n",
        "        attn = torch.bmm(q,k.permute(0,2,1)) # batch matrix multiplication\n",
        "        attn = attn * (C**(-0.5))  # sqrt(dk)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = torch.bmm(attn,v)\n",
        "\n",
        "        out = self.to_out(attn)\n",
        "        out = out.permute(0,2,1).view(B,C,H,W)\n",
        "\n",
        "        return out + residual\n",
        "\n",
        "class MidBlock(nn.Module):\n",
        "    def __init__(self,in_channels):\n",
        "        super(MidBlock, self).__init__()\n",
        "        self.res1 = ResNetBlock(in_channels,in_channels)\n",
        "        self.attn1 = AttentionBlock(in_channels)\n",
        "        self.res2 = ResNetBlock(in_channels,in_channels)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.res1(x)\n",
        "        x = self.attn1(x)\n",
        "        x = self.res2(x)\n",
        "        return x\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,has_attn=False):\n",
        "        super(DownBlock, self).__init__()\n",
        "        self.res1 = ResNetBlock(in_channels,out_channels)\n",
        "        self.res2 = ResNetBlock(out_channels,out_channels)\n",
        "        self.down = nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=2,padding=1)\n",
        "        if has_attn:\n",
        "            self.attn = AttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "    def forward(self,x):\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.attn(x)\n",
        "        x = self.down(x)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,in_channels=3,out_channels=4):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.inp = nn.Conv2d(in_channels,64,kernel_size=3,padding=1)\n",
        "        self.down_block = nn.Sequential(\n",
        "            DownBlock(64,128),\n",
        "            DownBlock(128,256),\n",
        "            DownBlock(256,512,has_attn = True)\n",
        "        )\n",
        "        self.bottle = MidBlock(512)\n",
        "        self.out = nn.Sequential(\n",
        "            nn.GroupNorm(32,512),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(512,out_channels*2,kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def reparameterize(self,x):\n",
        "        mean , log_var = torch.chunk(x,2,dim=1)\n",
        "        log_var = torch.clamp(log_var, -30.0, 20.0)\n",
        "        D_kl = 0.5 *(torch.exp(log_var) + mean**2 - log_var - 1)\n",
        "        D_kl = torch.sum(D_kl,dim=[1,2,3]).mean() # Mean is for batch dimention\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mean + eps*std\n",
        "        return z,D_kl\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.inp(x)\n",
        "        x = self.down_block(x)\n",
        "        x = self.bottle(x)\n",
        "        x = self.out(x)\n",
        "        z,D_kl = self.reparameterize(x)\n",
        "        return z,D_kl\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,has_attn=False):\n",
        "        super().__init__()\n",
        "        self.res1 = ResNetBlock(in_channels,out_channels)\n",
        "        self.res2 = ResNetBlock(out_channels,out_channels)\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2), # nearest mode by default\n",
        "            nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1)\n",
        "        )\n",
        "        if has_attn:\n",
        "            self.attn = AttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "    def forward(self,x):\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.attn(x)\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,in_channels=4,out_channels=3):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.inp = nn.Conv2d(in_channels,512,kernel_size=3,padding=1)\n",
        "        self.bottle = MidBlock(512)\n",
        "        self.up_block = nn.Sequential(\n",
        "            UpBlock(512,256,has_attn=True),\n",
        "            UpBlock(256,128),\n",
        "            UpBlock(128,64)\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.GroupNorm(32,64),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(64,out_channels,kernel_size=3,padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.inp(x)\n",
        "        x = self.bottle(x)\n",
        "        x = self.up_block(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "class PatchGan(nn.Module):\n",
        "    def __init__(self,in_channels=3):\n",
        "        super(PatchGan, self).__init__()\n",
        "        self.model=nn.Sequential(\n",
        "            nn.Conv2d(in_channels,64,kernel_size=3,stride=2,padding=1),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(64,128,kernel_size=3,stride=2,padding=1),\n",
        "            nn.GroupNorm(32,128),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(128,256,kernel_size=3,stride=2,padding=1),\n",
        "            nn.GroupNorm(32,256),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(256,512,kernel_size=3,stride=2,padding=1),\n",
        "            nn.GroupNorm(32,512),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(512,1,kernel_size=3,stride=1,padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self,in_channels=3,out_channels=4):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = Encoder(in_channels,out_channels)\n",
        "        self.decoder = Decoder(out_channels,in_channels)\n",
        "\n",
        "    def forward(self,x):\n",
        "        z,D_kl = self.encoder(x)\n",
        "        x = self.decoder(z)\n",
        "        return x,D_kl"
      ],
      "metadata": {
        "id": "3tFP3wtkebO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def raw_time_embedding(time , dim):\n",
        "    if not torch.is_tensor(time):\n",
        "        time = torch.tensor(time)\n",
        "\n",
        "    device=time.device\n",
        "    if time.ndim == 0:\n",
        "        time = time.unsqueeze(0).unsqueeze(1)\n",
        "    else: # This will be execute in training since t shape is (B)\n",
        "        time = time.unsqueeze(1)\n",
        "        # (B) -> (B,1)\n",
        "\n",
        "    # important to specify device\n",
        "    i=torch.arange(dim//2,device=device).float()\n",
        "    obj = (time)/(10000**(2*i/dim))\n",
        "    return torch.cat([torch.sin(obj),torch.cos(obj)],dim=1)\n",
        "\n",
        "class time_embedding(nn.Module):\n",
        "    def __init__(self,dim):\n",
        "        super().__init__()\n",
        "        self.net=nn.Sequential(\n",
        "            nn.Linear(dim,dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(dim,dim)\n",
        "        )\n",
        "\n",
        "    def forward(self,X):\n",
        "        return self.net(X)\n",
        "\n",
        "class DiffusionResNetBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,time_emb_dim,dropout_prob = 0.0 ):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.GroupNorm(32,in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1)\n",
        "        self.norm2 = nn.GroupNorm(32,out_channels)\n",
        "        self.drop = nn.Dropout(dropout_prob)\n",
        "        self.conv2 = nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1)\n",
        "        self.silu = nn.SiLU()\n",
        "\n",
        "        # purpose of this projection is to match channel dim , before adding to x\n",
        "        self.time_proj = nn.Linear(time_emb_dim,out_channels)\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels,out_channels,kernel_size=1)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self,x,time_emb):\n",
        "        x1 = x\n",
        "\n",
        "        x = self.norm1(x)\n",
        "        x = self.silu(x)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        # (B,C) -> (B,C,1,1)\n",
        "        # why silu ?\n",
        "        emb = self.time_proj(self.silu(time_emb))\n",
        "        x = x + emb[:, :, None, None]\n",
        "\n",
        "        x = self.norm2(x)\n",
        "        x = self.silu(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.conv2(x)\n",
        "        return x + self.shortcut(x1)\n",
        "\n",
        "class DiffusionAttentionBlock(nn.Module):\n",
        "    def __init__(self,in_channels):\n",
        "        super().__init__()\n",
        "        self.norm = nn.GroupNorm(32,in_channels)\n",
        "\n",
        "        self.to_q = nn.Linear(in_channels,in_channels)\n",
        "        self.to_k = nn.Linear(in_channels,in_channels)\n",
        "        self.to_v = nn.Linear(in_channels,in_channels)\n",
        "\n",
        "        self.to_out = nn.Linear(in_channels,in_channels)\n",
        "\n",
        "    def forward(self,x):\n",
        "        residual = x\n",
        "        B,C,H,W = x.shape\n",
        "        x = self.norm(x)\n",
        "        x = x.view(B,C,-1).permute(0,2,1)\n",
        "\n",
        "        q = self.to_q(x)\n",
        "        k = self.to_k(x)\n",
        "        v = self.to_v(x)\n",
        "\n",
        "        attn = torch.bmm(q,k.permute(0,2,1)) # batch matrix multiplication\n",
        "        attn = attn * (C**(-0.5))  # sqrt(dk)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = torch.bmm(attn,v)\n",
        "\n",
        "        out = self.to_out(attn)\n",
        "        out = out.permute(0,2,1).view(B,C,H,W)\n",
        "\n",
        "        return out + residual\n",
        "\n",
        "class DiffusionDownBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,time_emb_dim,has_attn=False):\n",
        "        super().__init__()\n",
        "        self.res1 = DiffusionResNetBlock(in_channels,out_channels,time_emb_dim)\n",
        "        self.res2 = DiffusionResNetBlock(out_channels,out_channels,time_emb_dim)\n",
        "        self.down = nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=2,padding=1)\n",
        "        if has_attn:\n",
        "            self.attn = DiffusionAttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "    def forward(self,x,time_emb):\n",
        "        x = self.res1(x,time_emb)\n",
        "        x = self.res2(x,time_emb)\n",
        "        x = self.attn(x)\n",
        "\n",
        "        skip_connection = x\n",
        "\n",
        "        x = self.down(x)\n",
        "        return x , skip_connection\n",
        "\n",
        "class DiffusionMidBlock(nn.Module):\n",
        "    def __init__(self,in_channels,time_emb_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.res1 = DiffusionResNetBlock(in_channels,in_channels,time_emb_dim)\n",
        "        self.attn = DiffusionAttentionBlock(in_channels)\n",
        "        self.res2 = DiffusionResNetBlock(in_channels,in_channels,time_emb_dim)\n",
        "\n",
        "    def forward(self,x,time_emb):\n",
        "        x = self.res1(x,time_emb)\n",
        "        x = self.attn(x)\n",
        "        x = self.res2(x,time_emb)\n",
        "        return x\n",
        "\n",
        "class DiffusionUpBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,time_emb_dim,has_attn=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(in_channels,in_channels,kernel_size=3,padding=1)\n",
        "        )\n",
        "        # Why input *2 ? Because we have to concatenate the channels\n",
        "        self.res1 = DiffusionResNetBlock(in_channels*2,out_channels,time_emb_dim)\n",
        "        self.res2 = DiffusionResNetBlock(out_channels,out_channels,time_emb_dim)\n",
        "        if has_attn:\n",
        "            self.attn = DiffusionAttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "    def forward(self,x,skip_connection,time_emb):\n",
        "        x = self.up(x)\n",
        "\n",
        "        # cancatenate at channels dimension\n",
        "        x = torch.cat([x,skip_connection],dim=1)\n",
        "        x = self.res1(x,time_emb)\n",
        "        x = self.res2(x,time_emb)\n",
        "        x = self.attn(x)\n",
        "        return x\n",
        "\n",
        "class DiffusionUnet(nn.Module):\n",
        "    def __init__(self,in_channels=4,out_channels=4,time_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.time_dim = time_dim\n",
        "        self.time_embedding = time_embedding(time_dim)\n",
        "\n",
        "        self.init_conv = nn.Conv2d(in_channels,64,kernel_size=3,padding=1)\n",
        "\n",
        "        self.down1 = DiffusionDownBlock(64,64,time_dim)\n",
        "        self.down2 = DiffusionDownBlock(64,128,time_dim)\n",
        "        self.down3 = DiffusionDownBlock(128,128,time_dim,has_attn=True)\n",
        "        self.down4 = DiffusionDownBlock(128,256,time_dim,has_attn=True)\n",
        "\n",
        "        self.mid = DiffusionMidBlock(256,time_dim)\n",
        "\n",
        "        self.up1 = DiffusionUpBlock(256,128,time_dim,has_attn=True)\n",
        "        self.up2 = DiffusionUpBlock(128,128,time_dim,has_attn=True)\n",
        "        self.up3 = DiffusionUpBlock(128,64,time_dim)\n",
        "        self.up4 = DiffusionUpBlock(64,64,time_dim)\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.GroupNorm(32,64),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(64,out_channels,kernel_size=3,padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self,x,t):\n",
        "        t = raw_time_embedding(t,self.time_dim)\n",
        "        emb = self.time_embedding(t)\n",
        "\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        x1 , skip1 = self.down1(x,emb)\n",
        "        x2 , skip2 = self.down2(x1,emb)\n",
        "        x3 , skip3 = self.down3(x2,emb)\n",
        "        x4 , skip4 = self.down4(x3,emb)\n",
        "\n",
        "        x = self.mid(x4,emb)\n",
        "\n",
        "        x = self.up1(x,skip4,emb)\n",
        "        x = self.up2(x,skip3,emb)\n",
        "        x = self.up3(x,skip2,emb)\n",
        "        x = self.up4(x,skip1,emb)\n",
        "\n",
        "        x = self.out(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "2hQ4VsCdelkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "repo_id = \"ziyang06315/latent_diffusion_from_scratch\"\n",
        "UNET_PATH = hf_hub_download(repo_id, filename=\"unet_epoch_500.pth\")\n",
        "VAE_CHECKPOINT = hf_hub_download(repo_id, filename=\"checkpoint.pth\")\n",
        "\n",
        "vae=VAE().to(DEVICE)\n",
        "checkpoint = torch.load(VAE_CHECKPOINT,map_location=DEVICE)\n",
        "vae.load_state_dict(checkpoint['vae_state_dict'])\n",
        "vae.eval()\n",
        "\n",
        "unet = DiffusionUnet().to(DEVICE)\n",
        "unet.load_state_dict(torch.load(UNET_PATH))\n",
        "unet.eval()\n",
        "\n",
        "timestep=250\n",
        "print(\"Start Processing\")\n",
        "with torch.no_grad():\n",
        "    blurry_latent = vae.encoder(blurry_img)[0]\n",
        "    noisy_latent = get_noisy_img(blurry_latent,timestep)\n",
        "    for t in reversed(range(timestep)):\n",
        "        t_tensor = torch.ones(1,device=DEVICE).long()*t\n",
        "\n",
        "        alpha = alphas[t]\n",
        "        alpha_cumprod = alphas_cumprod[t]\n",
        "        beta = betas[t]\n",
        "        sqrt_alpha_cumprod = sqrt_alphas_cumprod[t]\n",
        "        sqrt_one_minus_alpha_cumprod = sqrt_one_minus_alphas_cumprod[t]\n",
        "\n",
        "        if t>0:\n",
        "            noise = torch.randn_like(blurry_latent)\n",
        "        else:\n",
        "            noise = torch.zeros_like(blurry_latent)\n",
        "\n",
        "        noise_pred = unet(noisy_latent,t_tensor)\n",
        "\n",
        "        noisy_latent = (1 / torch.sqrt(alpha)) * (noisy_latent - ((1 - alpha) / (torch.sqrt(1 - alpha_cumprod))) * noise_pred) + torch.sqrt(beta) * noise\n",
        "\n",
        "    sharp_img = vae.decoder(noisy_latent)\n",
        "    sharp_img = plottable_clean_img(sharp_img)\n",
        "\n",
        "fig , (ax_dataset , ax_orig , ax_blur , ax_sharp) = plt.subplots(1,4,figsize=(20,5))\n",
        "\n",
        "ax_dataset.imshow(plottable_clean_img(dataset_img))\n",
        "ax_dataset.set_title(\"Dataset Image\")\n",
        "\n",
        "ax_orig.imshow(plottable_clean_img(noisy_img))\n",
        "ax_orig.set_title(\"Original Feed DIP Noised Image\")\n",
        "\n",
        "ax_blur.imshow(plottable_clean_img(blurry_img))\n",
        "ax_blur.set_title(\"Blurred Image\")\n",
        "\n",
        "ax_sharp.imshow(sharp_img)\n",
        "ax_sharp.set_title(\"Sharpened Image\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "_hadhODreoSA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18Dv0KjyPlPDbYGn5trrISFIJ12rSESSB",
      "authorship_tag": "ABX9TyMNrc3wSqmyJQfFRErKhq8Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}